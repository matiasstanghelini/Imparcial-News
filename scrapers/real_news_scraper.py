#!/usr/bin/env python3
"""
Scraper real de noticias argentinas - Obtiene noticias actuales de sitios web
"""

import requests
from bs4 import BeautifulSoup
import feedparser
import json
from datetime import datetime
import time
import re
import sys
import os

class RealArgentinianNewsScraper:
    def __init__(self):
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'es-ES,es;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1'
        }
        self.session = requests.Session()
        self.session.headers.update(self.headers)

    def scrape_lanacion(self):
        """Scraper para La Naci√≥n"""
        try:
            print("Scrapeando La Naci√≥n...")
            
            # Usar RSS feed de La Naci√≥n
            feed_url = "https://www.lanacion.com.ar/arcio/rss/"
            feed = feedparser.parse(feed_url)
            
            news_items = []
            for entry in feed.entries[:3]:
                try:
                    title = self.clean_text(entry.title)
                    summary = self.clean_text(entry.summary if hasattr(entry, 'summary') else entry.description)
                    
                    # Obtener fecha
                    if hasattr(entry, 'published_parsed') and entry.published_parsed:
                        date = datetime(*entry.published_parsed[:6]).strftime('%Y-%m-%d')
                    else:
                        date = datetime.now().strftime('%Y-%m-%d')
                    
                    news_item = {
                        'title': title[:120],
                        'summary': summary[:250], 
                        'source': 'La Naci√≥n',
                        'date': date,
                        'url': entry.link if hasattr(entry, 'link') else '',
                        'verdict': 'uncertain'
                    }
                    
                    news_items.append(news_item)
                    
                except Exception as e:
                    print(f"Error procesando entrada de La Naci√≥n: {e}")
                    continue
                    
            return news_items
            
        except Exception as e:
            print(f"Error scrapeando La Naci√≥n: {e}")
            return []

    def scrape_clarin(self):
        """Scraper para Clar√≠n"""
        try:
            print("Scrapeando Clar√≠n...")
            
            # Usar RSS feed de Clar√≠n
            feed_url = "https://www.clarin.com/rss.xml"
            feed = feedparser.parse(feed_url)
            
            news_items = []
            for entry in feed.entries[:3]:
                try:
                    title = self.clean_text(entry.title)
                    summary = self.clean_text(entry.summary if hasattr(entry, 'summary') else entry.description)
                    
                    # Obtener fecha
                    if hasattr(entry, 'published_parsed') and entry.published_parsed:
                        date = datetime(*entry.published_parsed[:6]).strftime('%Y-%m-%d')
                    else:
                        date = datetime.now().strftime('%Y-%m-%d')
                    
                    news_item = {
                        'title': title[:120],
                        'summary': summary[:250],
                        'source': 'Clar√≠n',
                        'date': date,
                        'url': entry.link if hasattr(entry, 'link') else '',
                        'verdict': 'uncertain'
                    }
                    
                    news_items.append(news_item)
                    
                except Exception as e:
                    print(f"Error procesando entrada de Clar√≠n: {e}")
                    continue
                    
            return news_items
            
        except Exception as e:
            print(f"Error scrapeando Clar√≠n: {e}")
            return []

    def scrape_infobae(self):
        """Scraper para Infobae"""
        try:
            print("Scrapeando Infobae...")
            
            # Usar RSS feed de Infobae
            feed_url = "https://www.infobae.com/feeds/rss/"
            feed = feedparser.parse(feed_url)
            
            news_items = []
            for entry in feed.entries[:3]:
                try:
                    title = self.clean_text(entry.title)
                    summary = self.clean_text(entry.summary if hasattr(entry, 'summary') else entry.description)
                    
                    # Obtener fecha
                    if hasattr(entry, 'published_parsed') and entry.published_parsed:
                        date = datetime(*entry.published_parsed[:6]).strftime('%Y-%m-%d')
                    else:
                        date = datetime.now().strftime('%Y-%m-%d')
                    
                    news_item = {
                        'title': title[:120],
                        'summary': summary[:250],
                        'source': 'Infobae',
                        'date': date,
                        'url': entry.link if hasattr(entry, 'link') else '',
                        'verdict': 'uncertain'
                    }
                    
                    news_items.append(news_item)
                    
                except Exception as e:
                    print(f"Error procesando entrada de Infobae: {e}")
                    continue
                    
            return news_items
            
        except Exception as e:
            print(f"Error scrapeando Infobae: {e}")
            return []

    def scrape_pagina12(self):
        """Scraper para P√°gina/12"""
        try:
            print("Scrapeando P√°gina/12...")
            
            # Usar RSS feed de P√°gina/12
            feed_url = "https://www.pagina12.com.ar/rss/portada"
            feed = feedparser.parse(feed_url)
            
            news_items = []
            for entry in feed.entries[:2]:
                try:
                    title = self.clean_text(entry.title)
                    summary = self.clean_text(entry.summary if hasattr(entry, 'summary') else entry.description)
                    
                    # Obtener fecha
                    if hasattr(entry, 'published_parsed') and entry.published_parsed:
                        date = datetime(*entry.published_parsed[:6]).strftime('%Y-%m-%d')
                    else:
                        date = datetime.now().strftime('%Y-%m-%d')
                    
                    news_item = {
                        'title': title[:120],
                        'summary': summary[:250],
                        'source': 'P√°gina/12',
                        'date': date,
                        'url': entry.link if hasattr(entry, 'link') else '',
                        'verdict': 'uncertain'
                    }
                    
                    news_items.append(news_item)
                    
                except Exception as e:
                    print(f"Error procesando entrada de P√°gina/12: {e}")
                    continue
                    
            return news_items
            
        except Exception as e:
            print(f"Error scrapeando P√°gina/12: {e}")
            return []

    def scrape_ambito(self):
        """Scraper para √Åmbito"""
        try:
            print("Scrapeando √Åmbito...")
            
            # Usar RSS feed de √Åmbito
            feed_url = "https://www.ambito.com/rss/home.xml"
            feed = feedparser.parse(feed_url)
            
            news_items = []
            for entry in feed.entries[:2]:
                try:
                    title = self.clean_text(entry.title)
                    summary = self.clean_text(entry.summary if hasattr(entry, 'summary') else entry.description)
                    
                    # Obtener fecha
                    if hasattr(entry, 'published_parsed') and entry.published_parsed:
                        date = datetime(*entry.published_parsed[:6]).strftime('%Y-%m-%d')
                    else:
                        date = datetime.now().strftime('%Y-%m-%d')
                    
                    news_item = {
                        'title': title[:120],
                        'summary': summary[:250],
                        'source': '√Åmbito',
                        'date': date,
                        'url': entry.link if hasattr(entry, 'link') else '',
                        'verdict': 'uncertain'
                    }
                    
                    news_items.append(news_item)
                    
                except Exception as e:
                    print(f"Error procesando entrada de √Åmbito: {e}")
                    continue
                    
            return news_items
            
        except Exception as e:
            print(f"Error scrapeando √Åmbito: {e}")
            return []

    def clean_text(self, text):
        """Limpia y normaliza el texto"""
        if not text:
            return ""
        
        # Remover HTML tags
        text = re.sub(r'<[^>]+>', '', text)
        # Normalizar espacios
        text = re.sub(r'\s+', ' ', text)
        # Limpiar caracteres especiales
        text = text.replace('\n', ' ').replace('\r', ' ').replace('\t', ' ')
        # Remover entidades HTML comunes
        text = text.replace('&amp;', '&').replace('&lt;', '<').replace('&gt;', '>')
        text = text.replace('&quot;', '"').replace('&#39;', "'")
        
        return text.strip()

    def scrape_all_sources(self):
        """Scrape todas las fuentes reales"""
        all_news = []
        
        # Lista de funciones de scraping
        scrapers = [
            self.scrape_lanacion,
            self.scrape_clarin,
            self.scrape_infobae,
            self.scrape_pagina12,
            self.scrape_ambito
        ]
        
        for scraper_func in scrapers:
            try:
                news_items = scraper_func()
                all_news.extend(news_items)
                time.sleep(2)  # Pausa entre scrapers para ser respetuosos
            except Exception as e:
                print(f"Error en scraper {scraper_func.__name__}: {e}")
                continue
        
        # Filtrar noticias v√°lidas
        valid_news = []
        for news in all_news:
            if news['title'] and len(news['title']) > 10 and news['summary']:
                valid_news.append(news)
        
        # Ordenar por fecha (m√°s recientes primero)
        valid_news = sorted(valid_news, key=lambda x: x['date'], reverse=True)
        
        return valid_news[:12]  # Limitar a 12 noticias

    def save_to_json(self, news_data, filename):
        """Guarda las noticias en formato JSON"""
        try:
            # Agregar IDs √∫nicos y estructura de agentes
            for i, news in enumerate(news_data, 1):
                news['id'] = i
                news['agents'] = {
                    'logic': f'Noticia obtenida de {news["source"]} - Requiere verificaci√≥n de fuentes primarias',
                    'context': f'Informaci√≥n publicada el {news["date"]} - An√°lisis contextual pendiente',
                    'expert': 'An√°lisis t√©cnico especializado pendiente - Evaluaci√≥n de credibilidad requerida',
                    'synth': [
                        f'üì∞ Fuente: {news["source"]}',
                        f'üìÖ Fecha: {news["date"]}',
                        f'üîó URL: Disponible',
                        'üîç Estado: Noticia real obtenida'
                    ]
                }
            
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(news_data, f, ensure_ascii=False, indent=2)
            
            print(f"‚úì Noticias reales guardadas en {filename}")
            return True
            
        except Exception as e:
            print(f"Error guardando archivo: {e}")
            return False

def main():
    """Funci√≥n principal para ejecutar el scraper real"""
    scraper = RealArgentinianNewsScraper()
    
    print("=== Iniciando scraping REAL de noticias argentinas ===")
    
    try:
        news_data = scraper.scrape_all_sources()
        
        if news_data:
            print(f"\n‚úì Se obtuvieron {len(news_data)} noticias reales")
            
            # Guardar en archivo para la aplicaci√≥n
            output_file = os.path.join(os.path.dirname(__file__), '..', 'data', 'real_news.json')
            scraper.save_to_json(news_data, output_file)
            
            # Mostrar muestra de noticias obtenidas
            print("\n--- MUESTRA DE NOTICIAS REALES ---")
            for i, news in enumerate(news_data[:3], 1):
                print(f"\n{i}. {news['source']}")
                print(f"   T√≠tulo: {news['title']}")
                print(f"   Fecha: {news['date']}")
                print(f"   Resumen: {news['summary'][:80]}...")
            
            return True
            
        else:
            print("‚ùå No se pudieron obtener noticias reales")
            return False
            
    except Exception as e:
        print(f"‚ùå Error general en el scraper: {e}")
        return False

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)